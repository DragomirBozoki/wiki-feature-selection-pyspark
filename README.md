# ğŸ§  Wikipedia Topic Classification

A modular pipeline for extracting, cleaning, processing, and classifying Wikipedia articles into predefined topics using **Apache Spark** and optionally **Apache Airflow** for orchestration.

---

## ğŸ”§ Features

- âœ… Extract and parse Wikipedia XML dump using `wikiextractor`
- âœ… Clean and tokenize large-scale text data with PySpark
- âœ… Automatically assign semantic labels using keyword-based lemmatized matching
- âœ… Generate unigrams, bigrams, trigrams for each article
- âœ… Compute entropy and mutual information for informative feature selection
- âœ… Train classification models using Spark ML
- âœ… Orchestrate tasks with Apache Airflow (PythonOperator or BashOperator-based DAGs)

---

## ğŸ“ Project Structure

BigData/
â”œâ”€â”€ extracted/ # Extracted JSON articles from Wikipedia
â”œâ”€â”€ data/ # Cached intermediate data (Parquet, .txt)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocessing.py      # Tokenization, entropy, and word count
â”‚ â”œâ”€â”€ n_grams.py            # MI calculation and n-gram generation
â”‚ â”œâ”€â”€ unigrams.py           # Basic word frequency features
â”‚ â”œâ”€â”€ classification.py     # Classification pipeline
â”‚ â”œâ”€â”€ label.py              # Text labeling via lemmatized topic detection
â”‚ â””â”€â”€ pipeline_steps.py     # Modular step functions for DAG integration
â”œâ”€â”€ main.py                 # Standalone script to run the full pipeline
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ bigdata_pipeline.py   # Airflow DAG using PythonOperators
â”‚ â””â”€â”€ bash_pipeline.py      # Simple BashOperator DAG (runs main.py)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ“¥ Input

- Wikipedia XML dump:  
  `enwiki-latest-pages-articles.xml`

- Use [`wikiextractor`](https://github.com/attardi/wikiextractor) to convert the XML dump to JSON format.

```bash
wikiextractor enwiki-latest-pages-articles.xml -o extracted --json
```

---
## âš™ï¸ How to Run
â–¶ï¸ Manually (without Airflow)

# Activate your environment
source venv-bigdata/bin/activate

# Run the full pipeline
python main.py

---

## ğŸŒ€ With Airflow
Option 1: Modular DAG with PythonOperators

# Trigger manually from Airflow UI or CLI
airflow dags trigger bigdata_pipeline_dag

Option 2: Bash DAG (runs main.py)

airflow dags trigger bigdata_wiki_pipeline

Labeled articles based on lemmatized word sets into these domains:

    science, technology, politics, sports, health, business

    entertainment, history, geography, military

    New: education, religion, environment

    Articles that donâ€™t fit: other

---
## ğŸ“Š Outputs

    Entropy scores for each n-gram level

    Top 500 Mutual Information features (per n-gram)

    Labeled Spark DataFrames

    Trained classification models

    Accuracy printed for each model (unigram, bigram, trigram)

---
## ğŸ“Œ Requirements

    Python â‰¥ 3.8

    Apache Spark

    Apache Airflow (optional)

    nltk (for lemmatization)

    PySpark ML

# Install with:
```bash
pip install -r requirements.txt
```
## ğŸ‘¨â€ğŸ’» Author

Dragomir Bozoki
Machine Learning | Big Data pipelines | NLP with Spark

---

## ğŸ“œ License

MIT â€“ feel free to use, extend, or fork for your own NLP pipelines.
